{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created file '/Users/jchien/workspace/courses/coursera_ml/ex4/octave/libs/displayData.m'.\n"
     ]
    }
   ],
   "source": [
    "%%file ../libs/displayData.m\n",
    "function [h, display_array] = displayData(X, example_width)\n",
    "    if ~exist('example_width', 'var') || isempty(example_width)\n",
    "        example_width = round(sqrt(size(X, 2)));\n",
    "    end\n",
    "\n",
    "    colormap(gray)\n",
    "    [m, n] = size(X);\n",
    "\n",
    "    example_height = (n / example_width);\n",
    "\n",
    "    display_rows = floor(sqrt(m));\n",
    "    display_cols = ceil(m/display_rows);\n",
    "\n",
    "    % between images padding\n",
    "    pad = 1;\n",
    "\n",
    "    display_array = - ones(pad + display_rows * (example_height + pad), ...\n",
    "                           pad + display_cols * (example_width + pad));\n",
    "\n",
    "    curr_ex = 1;\n",
    "    for j = 1:display_rows\n",
    "        for i = 1:display_cols\n",
    "            if curr_ex > m,\n",
    "                break;\n",
    "            end\n",
    "\n",
    "            max_val = max(abs(X(curr_ex, :)));\n",
    "            display_array(pad + (j-1) * (example_height + pad) + (1:example_height), ...\n",
    "                          pad + (i-1) * (example_width + pad) + (1:example_width)) = ...\n",
    "                            reshape(X(curr_ex, :), example_height, example_width) / max_val;\n",
    "            curr_ex = curr_ex + 1;\n",
    "        end\n",
    "        if curr_ex > m,\n",
    "            break;\n",
    "        end\n",
    "    end\n",
    "\n",
    "    h = imagesc(display_array, [-1 1]);\n",
    "    % do not show axis\n",
    "    axis image off\n",
    "    drawnow;\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created file '/Users/jchien/workspace/courses/coursera_ml/ex4/octave/libs/sigmoid.m'.\n"
     ]
    }
   ],
   "source": [
    "%%file ../libs/sigmoid.m\n",
    "function g = sigmoid(z)\n",
    "    g = 1.0 ./ (1.0 + exp(-z));\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created file '/Users/jchien/workspace/courses/coursera_ml/ex4/octave/libs/sigmoidGradient.m'.\n"
     ]
    }
   ],
   "source": [
    "%%file ../libs/sigmoidGradient.m\n",
    "function gd = sigmoidGradient(z)\n",
    "    gd = sigmoid(z) .* (1 - sigmoid(z));\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created file '/Users/jchien/workspace/courses/coursera_ml/ex4/octave/libs/hypothesis.m'.\n"
     ]
    }
   ],
   "source": [
    "%%file ../libs/hypothesis.m\n",
    "function HL = hypothesis(Theta1, Theta2, X)\n",
    "    m = size(X, 1);\n",
    "    a1 = [ones(m, 1) X];\n",
    "\n",
    "    z2 = a1 * Theta1';\n",
    "    a2 = [ones(m, 1) sigmoid(z2)];\n",
    "\n",
    "    z3 = a2 * Theta2';\n",
    "    a3 = sigmoid(z3);\n",
    "\n",
    "    HL = [a2(:); a3(:)];\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created file '/Users/jchien/workspace/courses/coursera_ml/ex4/octave/libs/costFunction.m'.\n"
     ]
    }
   ],
   "source": [
    "%%file ../libs/costFunction.m\n",
    "function J = costFunction(Theta1, Theta2, g, Y, lambda)\n",
    "    m = size(Y, 1);\n",
    "\n",
    "    J = sum(sum(Y .* log(g) + (1-Y).*log(1-g)))*(-1/m);\n",
    "    tempTheta1 = Theta1(:, 2:end);\n",
    "    tempTheta2 = Theta2(:, 2:end);\n",
    "    reg = (lambda / (2*m)) * sum([sum(sum(tempTheta1 .^ 2)) sum(sum(tempTheta2 .^2))]);\n",
    "    J = J + reg;\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created file '/Users/jchien/workspace/courses/coursera_ml/ex4/octave/libs/nnCostFunction.m'.\n"
     ]
    }
   ],
   "source": [
    "%%file ../libs/nnCostFunction.m\n",
    "function [J grad] = nnCostFunction(nn_params, ...\n",
    "                        input_layer_size, ...\n",
    "                        hidden_layer_size, ...\n",
    "                        num_labels, ...\n",
    "                        X, y, lambda)\n",
    "\n",
    "    Theta1 = reshape(nn_params(1:hidden_layer_size * (input_layer_size + 1)), ...\n",
    "                    hidden_layer_size, (input_layer_size + 1));\n",
    "    Theta2 = reshape(nn_params(1 + hidden_layer_size * (input_layer_size + 1): end), ...\n",
    "                    num_labels, (hidden_layer_size + 1));\n",
    "\n",
    "    m = size(X, 1);\n",
    "\n",
    "    Y = zeros(m, num_labels);\n",
    "    for i=1:m\n",
    "        Y(i, y(i, 1)) = 1;\n",
    "    end\n",
    "\n",
    "    # forward propagation\n",
    "    a1 = [ones(m, 1) X];\n",
    "\n",
    "    z2 = a1 * Theta1';\n",
    "    a2 = [ones(m, 1) sigmoid(z2)];\n",
    "\n",
    "    z3 = a2 * Theta2';\n",
    "    a3 = sigmoid(z3);\n",
    "\n",
    "    J = costFunction(Theta1, Theta2, a3, Y, lambda);\n",
    "\n",
    "    grad = nnGradFunction(Theta1, Theta2, a1, a2, a3, Y, lambda);\n",
    "\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created file '/Users/jchien/workspace/courses/coursera_ml/ex4/octave/libs/nnGradFunction.m'.\n"
     ]
    }
   ],
   "source": [
    "%%file ../libs/nnGradFunction.m\n",
    "function grad = nnGradFunction(Theta1, Theta2, a1, a2, a3, Y, lambda)\n",
    "    m = size(Y, 1);\n",
    "\n",
    "    Theta1_grad = zeros(size(Theta1));\n",
    "    Theta2_grad = zeros(size(Theta2));\n",
    "\n",
    "    D2 = zeros(size(Theta2_grad));\n",
    "    D1 = zeros(size(Theta1_grad));\n",
    "\n",
    "    grad = 0;\n",
    "\n",
    "    % y = 5000 x 10\n",
    "    % a3 = 5000 x 10\n",
    "    % a2 = 5000 x 26\n",
    "    % a1 = 5000 x 401\n",
    "    % Theta2 = 10 x 26\n",
    "    % Theta1 = 25 x 401\n",
    "\n",
    "    d3 = a3 - Y; % 5000 x 10\n",
    "    d2 = d3 * Theta2 .* (a2 .* (1 - a2));\n",
    "    d2 = d2(:, 2:end); % 5000 x 25\n",
    "    D2 = d3' * a2; % 10 x 26\n",
    "    D1 = d2' * a1; % 25 x 401\n",
    "\n",
    "    tempTheta1 = [zeros(size(Theta1, 1), 1) Theta1(:, 2:end)];\n",
    "    tempTheta2 = [zeros(size(Theta2, 1), 1) Theta2(:, 2:end)];\n",
    "\n",
    "    Theta2_grad = D2./m + (lambda/m) * tempTheta2;\n",
    "    Theta1_grad = D1./m + (lambda/m) * tempTheta1;\n",
    "\n",
    "    grad = [Theta1_grad(:); Theta2_grad(:)];\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created file '/Users/jchien/workspace/courses/coursera_ml/ex4/octave/libs/predict.m'.\n"
     ]
    }
   ],
   "source": [
    "%%file ../libs/predict.m\n",
    "function p = predict(Theta1, Theta2, X)\n",
    "    m = size(X, 1);\n",
    "    num_labels = size(Theta2, 1);\n",
    "\n",
    "    a1 = [ones(m, 1) X];\n",
    "\n",
    "    z2 = a1 * Theta1';\n",
    "    a2 = [ones(m, 1) sigmoid(z2)];\n",
    "\n",
    "    z3 = a2 * Theta2';\n",
    "    a3 = sigmoid(z3);\n",
    "\n",
    "    [value, index] = max(a3, [], 2);\n",
    "    p = index;\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random initialization\n",
    "\n",
    "important for training NN for **symmetry breaking**\n",
    "\n",
    "one effective strategy is select values for $\\Theta^{(l)}$ uniformly in range $[-\\epsilon, \\epsilon]$\n",
    "\n",
    "A good choice for $\\epsilon$ is\n",
    "\n",
    "$$\n",
    "\\epsilon = \\frac{\\sqrt{6}}{\\sqrt{L_{in} + L_{out}}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created file '/Users/jchien/workspace/courses/coursera_ml/ex4/octave/libs/randInitializeWeights.m'.\n"
     ]
    }
   ],
   "source": [
    "%%file ../libs/randInitializeWeights.m\n",
    "function W = randInitializeWeights(L_in, L_out)\n",
    "    W = zeros(L_out, 1 + L_in);\n",
    "    episilon_init = 0.12;\n",
    "    W = rand(L_out, 1 + L_in) * 2 * episilon_init - episilon_init;\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created file '/Users/jchien/workspace/courses/coursera_ml/ex4/octave/libs/debugInitializeWeights.m'.\n"
     ]
    }
   ],
   "source": [
    "%%file ../libs/debugInitializeWeights.m\n",
    "function W = debugInitializeWeights(fan_out, fan_in)\n",
    "    %DEBUGINITIALIZEWEIGHTS Initialize the weights of a layer with fan_in\n",
    "    %incoming connections and fan_out outgoing connections using a fixed\n",
    "    %strategy, this will help you later in debugging\n",
    "    %   W = DEBUGINITIALIZEWEIGHTS(fan_in, fan_out) initializes the weights \n",
    "    %   of a layer with fan_in incoming connections and fan_out outgoing \n",
    "    %   connections using a fix set of values\n",
    "    %\n",
    "    %   Note that W should be set to a matrix of size(1 + fan_in, fan_out) as\n",
    "    %   the first row of W handles the \"bias\" terms\n",
    "    %\n",
    "\n",
    "    % Set W to zeros\n",
    "    W = zeros(fan_out, 1 + fan_in);\n",
    "\n",
    "    % Initialize W using \"sin\", this ensures that W is always of the same\n",
    "    % values and will be useful for debugging\n",
    "    W = reshape(sin(1:numel(W)), size(W)) / 10;\n",
    "\n",
    "    % =========================================================================\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created file '/Users/jchien/workspace/courses/coursera_ml/ex4/octave/libs/computeNumericalGradient.m'.\n"
     ]
    }
   ],
   "source": [
    "%%file ../libs/computeNumericalGradient.m\n",
    "function numgrad = computeNumericalGradient(J, theta)\n",
    "    %COMPUTENUMERICALGRADIENT Computes the gradient using \"finite differences\"\n",
    "    %and gives us a numerical estimate of the gradient.\n",
    "    %   numgrad = COMPUTENUMERICALGRADIENT(J, theta) computes the numerical\n",
    "    %   gradient of the function J around theta. Calling y = J(theta) should\n",
    "    %   return the function value at theta.\n",
    "\n",
    "    % Notes: The following code implements numerical gradient checking, and \n",
    "    %        returns the numerical gradient.It sets numgrad(i) to (a numerical \n",
    "    %        approximation of) the partial derivative of J with respect to the \n",
    "    %        i-th input argument, evaluated at theta. (i.e., numgrad(i) should \n",
    "    %        be the (approximately) the partial derivative of J with respect \n",
    "    %        to theta(i).)\n",
    "    %                \n",
    "\n",
    "    numgrad = zeros(size(theta));\n",
    "    perturb = zeros(size(theta));\n",
    "    e = 1e-4;\n",
    "    for p = 1:numel(theta)\n",
    "        % Set perturbation vector\n",
    "        perturb(p) = e;\n",
    "        loss1 = J(theta - perturb);\n",
    "        loss2 = J(theta + perturb);\n",
    "        % Compute Numerical Gradient\n",
    "        numgrad(p) = (loss2 - loss1) / (2*e);\n",
    "        perturb(p) = 0;\n",
    "    end\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created file '/Users/jchien/workspace/courses/coursera_ml/ex4/octave/libs/checkNNGradients.m'.\n"
     ]
    }
   ],
   "source": [
    "%%file ../libs/checkNNGradients.m\n",
    "function checkNNGradients(lambda)\n",
    "    %CHECKNNGRADIENTS Creates a small neural network to check the\n",
    "    %backpropagation gradients\n",
    "    %   CHECKNNGRADIENTS(lambda) Creates a small neural network to check the\n",
    "    %   backpropagation gradients, it will output the analytical gradients\n",
    "    %   produced by your backprop code and the numerical gradients (computed\n",
    "    %   using computeNumericalGradient). These two gradient computations should\n",
    "    %   result in very similar values.\n",
    "    %\n",
    "\n",
    "    if ~exist('lambda', 'var') || isempty(lambda)\n",
    "        lambda = 0;\n",
    "    end\n",
    "\n",
    "    input_layer_size = 3;\n",
    "    hidden_layer_size = 5;\n",
    "    num_labels = 3;\n",
    "    m = 5;\n",
    "\n",
    "    % We generate some 'random' test data\n",
    "    Theta1 = debugInitializeWeights(hidden_layer_size, input_layer_size);\n",
    "    Theta2 = debugInitializeWeights(num_labels, hidden_layer_size);\n",
    "    % Reusing debugInitializeWeights to generate X\n",
    "    X  = debugInitializeWeights(m, input_layer_size - 1);\n",
    "    y  = 1 + mod(1:m, num_labels)';\n",
    "\n",
    "    % Unroll parameters\n",
    "    nn_params = [Theta1(:) ; Theta2(:)];\n",
    "\n",
    "    % Short hand for cost function\n",
    "    costFunc = @(p) nnCostFunction(p, input_layer_size, hidden_layer_size, ...\n",
    "                                num_labels, X, y, lambda);\n",
    "\n",
    "    [cost, grad] = costFunc(nn_params);\n",
    "\n",
    "    numgrad = computeNumericalGradient(costFunc, nn_params);\n",
    "\n",
    "    % Visually examine the two gradient computations.  The two columns\n",
    "    % you get should be very similar. \n",
    "    disp([numgrad grad]);\n",
    "    fprintf(['The above two columns you get should be very similar.\\n' ...\n",
    "            '(Left-Your Numerical Gradient, Right-Analytical Gradient)\\n\\n']);\n",
    "\n",
    "    % Evaluate the norm of the difference between two solutions.  \n",
    "    % If you have a correct implementation, and assuming you used EPSILON = 0.0001 \n",
    "    % in computeNumericalGradient.m, then diff below should be less than 1e-9\n",
    "    diff = norm(numgrad-grad)/norm(numgrad+grad);\n",
    "\n",
    "    fprintf(['If your backpropagation implementation is correct, then \\n' ...\n",
    "            'the relative difference will be small (less than 1e-9). \\n' ...\n",
    "            '\\nRelative Difference: %g\\n'], diff);\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created file '/Users/jchien/workspace/courses/coursera_ml/ex4/octave/libs/fmincg.m'.\n"
     ]
    }
   ],
   "source": [
    "%%file ../libs/fmincg.m\n",
    "function [X, fX, i] = fmincg(f, X, options, P1, P2, P3, P4, P5)\n",
    "    % Minimize a continuous differentialble multivariate function. Starting point\n",
    "    % is given by \"X\" (D by 1), and the function named in the string \"f\", must\n",
    "    % return a function value and a vector of partial derivatives. The Polack-\n",
    "    % Ribiere flavour of conjugate gradients is used to compute search directions,\n",
    "    % and a line search using quadratic and cubic polynomial approximations and the\n",
    "    % Wolfe-Powell stopping criteria is used together with the slope ratio method\n",
    "    % for guessing initial step sizes. Additionally a bunch of checks are made to\n",
    "    % make sure that exploration is taking place and that extrapolation will not\n",
    "    % be unboundedly large. The \"length\" gives the length of the run: if it is\n",
    "    % positive, it gives the maximum number of line searches, if negative its\n",
    "    % absolute gives the maximum allowed number of function evaluations. You can\n",
    "    % (optionally) give \"length\" a second component, which will indicate the\n",
    "    % reduction in function value to be expected in the first line-search (defaults\n",
    "    % to 1.0). The function returns when either its length is up, or if no further\n",
    "    % progress can be made (ie, we are at a minimum, or so close that due to\n",
    "    % numerical problems, we cannot get any closer). If the function terminates\n",
    "    % within a few iterations, it could be an indication that the function value\n",
    "    % and derivatives are not consistent (ie, there may be a bug in the\n",
    "    % implementation of your \"f\" function). The function returns the found\n",
    "    % solution \"X\", a vector of function values \"fX\" indicating the progress made\n",
    "    % and \"i\" the number of iterations (line searches or function evaluations,\n",
    "    % depending on the sign of \"length\") used.\n",
    "    %\n",
    "    % Usage: [X, fX, i] = fmincg(f, X, options, P1, P2, P3, P4, P5)\n",
    "    %\n",
    "    % See also: checkgrad \n",
    "    %\n",
    "    % Copyright (C) 2001 and 2002 by Carl Edward Rasmussen. Date 2002-02-13\n",
    "    %\n",
    "    %\n",
    "    % (C) Copyright 1999, 2000 & 2001, Carl Edward Rasmussen\n",
    "    % \n",
    "    % Permission is granted for anyone to copy, use, or modify these\n",
    "    % programs and accompanying documents for purposes of research or\n",
    "    % education, provided this copyright notice is retained, and note is\n",
    "    % made of any changes that have been made.\n",
    "    % \n",
    "    % These programs and documents are distributed without any warranty,\n",
    "    % express or implied.  As the programs were written for research\n",
    "    % purposes only, they have not been tested to the degree that would be\n",
    "    % advisable in any important application.  All use of these programs is\n",
    "    % entirely at the user's own risk.\n",
    "    %\n",
    "    % [ml-class] Changes Made:\n",
    "    % 1) Function name and argument specifications\n",
    "    % 2) Output display\n",
    "    %\n",
    "\n",
    "    % Read options\n",
    "    if exist('options', 'var') && ~isempty(options) && isfield(options, 'MaxIter')\n",
    "        length = options.MaxIter;\n",
    "    else\n",
    "        length = 100;\n",
    "    end\n",
    "\n",
    "\n",
    "    RHO = 0.01;                            % a bunch of constants for line searches\n",
    "    SIG = 0.5;       % RHO and SIG are the constants in the Wolfe-Powell conditions\n",
    "    INT = 0.1;    % don't reevaluate within 0.1 of the limit of the current bracket\n",
    "    EXT = 3.0;                    % extrapolate maximum 3 times the current bracket\n",
    "    MAX = 20;                         % max 20 function evaluations per line search\n",
    "    RATIO = 100;                                      % maximum allowed slope ratio\n",
    "\n",
    "    argstr = ['feval(f, X'];                      % compose string used to call function\n",
    "    for i = 1:(nargin - 3)\n",
    "        argstr = [argstr, ',P', int2str(i)];\n",
    "    end\n",
    "    argstr = [argstr, ')'];\n",
    "\n",
    "    if max(size(length)) == 2, red=length(2); length=length(1); else red=1; end\n",
    "    S=['Iteration '];\n",
    "\n",
    "    i = 0;                                            % zero the run length counter\n",
    "    ls_failed = 0;                             % no previous line search has failed\n",
    "    fX = [];\n",
    "    [f1 df1] = eval(argstr);                      % get function value and gradient\n",
    "    i = i + (length<0);                                            % count epochs?!\n",
    "    s = -df1;                                        % search direction is steepest\n",
    "    d1 = -s'*s;                                                 % this is the slope\n",
    "    z1 = red/(1-d1);                                  % initial step is red/(|s|+1)\n",
    "\n",
    "    while i < abs(length)                                      % while not finished\n",
    "        i = i + (length>0);                                      % count iterations?!\n",
    "\n",
    "        X0 = X; f0 = f1; df0 = df1;                   % make a copy of current values\n",
    "        X = X + z1*s;                                             % begin line search\n",
    "        [f2 df2] = eval(argstr);\n",
    "        i = i + (length<0);                                          % count epochs?!\n",
    "        d2 = df2'*s;\n",
    "        f3 = f1; d3 = d1; z3 = -z1;             % initialize point 3 equal to point 1\n",
    "        if length>0, M = MAX; else M = min(MAX, -length-i); end\n",
    "        success = 0; limit = -1;                     % initialize quanteties\n",
    "        while 1\n",
    "            while ((f2 > f1+z1*RHO*d1) || (d2 > -SIG*d1)) && (M > 0) \n",
    "                limit = z1;                                         % tighten the bracket\n",
    "                if f2 > f1\n",
    "                  z2 = z3 - (0.5*d3*z3*z3)/(d3*z3+f2-f3);                 % quadratic fit\n",
    "                else\n",
    "                  A = 6*(f2-f3)/z3+3*(d2+d3);                                 % cubic fit\n",
    "                  B = 3*(f3-f2)-z3*(d3+2*d2);\n",
    "                  z2 = (sqrt(B*B-A*d2*z3*z3)-B)/A;       % numerical error possible - ok!\n",
    "                end\n",
    "                if isnan(z2) || isinf(z2)\n",
    "                  z2 = z3/2;                  % if we had a numerical problem then bisect\n",
    "                end\n",
    "                z2 = max(min(z2, INT*z3),(1-INT)*z3);  % don't accept too close to limits\n",
    "                z1 = z1 + z2;                                           % update the step\n",
    "                X = X + z2*s;\n",
    "                [f2 df2] = eval(argstr);\n",
    "                M = M - 1; i = i + (length<0);                           % count epochs?!\n",
    "                d2 = df2'*s;\n",
    "                z3 = z3-z2;                    % z3 is now relative to the location of z2\n",
    "            end\n",
    "            if f2 > f1+z1*RHO*d1 || d2 > -SIG*d1\n",
    "                break;                                                % this is a failure\n",
    "            elseif d2 > SIG*d1\n",
    "                success = 1; break;                                             % success\n",
    "            elseif M == 0\n",
    "                break;                                                          % failure\n",
    "            end\n",
    "            A = 6*(f2-f3)/z3+3*(d2+d3);                      % make cubic extrapolation\n",
    "            B = 3*(f3-f2)-z3*(d3+2*d2);\n",
    "            z2 = -d2*z3*z3/(B+sqrt(B*B-A*d2*z3*z3));        % num. error possible - ok!\n",
    "            if ~isreal(z2) || isnan(z2) || isinf(z2) || z2 < 0   % num prob or wrong sign?\n",
    "                if limit < -0.5                               % if we have no upper limit\n",
    "                    z2 = z1 * (EXT-1);                 % the extrapolate the maximum amount\n",
    "                else\n",
    "                    z2 = (limit-z1)/2;                                   % otherwise bisect\n",
    "                end\n",
    "            elseif (limit > -0.5) && (z2+z1 > limit)          % extraplation beyond max?\n",
    "                z2 = (limit-z1)/2;                                               % bisect\n",
    "            elseif (limit < -0.5) && (z2+z1 > z1*EXT)       % extrapolation beyond limit\n",
    "                z2 = z1*(EXT-1.0);                           % set to extrapolation limit\n",
    "            elseif z2 < -z3*INT\n",
    "                z2 = -z3*INT;\n",
    "            elseif (limit > -0.5) && (z2 < (limit-z1)*(1.0-INT))   % too close to limit?\n",
    "                z2 = (limit-z1)*(1.0-INT);\n",
    "            end\n",
    "            f3 = f2; d3 = d2; z3 = -z2;                  % set point 3 equal to point 2\n",
    "            z1 = z1 + z2; X = X + z2*s;                      % update current estimates\n",
    "            [f2 df2] = eval(argstr);\n",
    "            M = M - 1; i = i + (length<0);                             % count epochs?!\n",
    "            d2 = df2'*s;\n",
    "        end                                                      % end of line search\n",
    "\n",
    "        if success                                         % if line search succeeded\n",
    "            f1 = f2; fX = [fX' f1]';\n",
    "            fprintf('%s %4i | Cost: %4.6e\\r', S, i, f1);\n",
    "            s = (df2'*df2-df1'*df2)/(df1'*df1)*s - df2;      % Polack-Ribiere direction\n",
    "            tmp = df1; df1 = df2; df2 = tmp;                         % swap derivatives\n",
    "            d2 = df1'*s;\n",
    "            if d2 > 0                                      % new slope must be negative\n",
    "                s = -df1;                              % otherwise use steepest direction\n",
    "                d2 = -s'*s;    \n",
    "            end\n",
    "            z1 = z1 * min(RATIO, d1/(d2-realmin));          % slope ratio but max RATIO\n",
    "            d1 = d2;\n",
    "            ls_failed = 0;                              % this line search did not fail\n",
    "        else\n",
    "          X = X0; f1 = f0; df1 = df0;  % restore point from before failed line search\n",
    "          if ls_failed || i > abs(length)          % line search failed twice in a row\n",
    "              break;                             % or we ran out of time, so we give up\n",
    "          end\n",
    "          tmp = df1; df1 = df2; df2 = tmp;                         % swap derivatives\n",
    "          s = -df1;                                                    % try steepest\n",
    "          d1 = -s'*s;\n",
    "          z1 = 1/(1-d1);                     \n",
    "          ls_failed = 1;                                    % this line search failed\n",
    "        end\n",
    "        if exist('OCTAVE_VERSION')\n",
    "            fflush(stdout);\n",
    "        end\n",
    "      end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Octave",
   "language": "octave",
   "name": "octave"
  },
  "language_info": {
   "file_extension": ".m",
   "help_links": [
    {
     "text": "GNU Octave",
     "url": "https://www.gnu.org/software/octave/support.html"
    },
    {
     "text": "Octave Kernel",
     "url": "https://github.com/Calysto/octave_kernel"
    },
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-octave",
   "name": "octave",
   "version": "7.1.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
